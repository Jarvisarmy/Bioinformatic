{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "300413cd",
   "metadata": {},
   "source": [
    "https://github.com/SenticNet/context2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73e7a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'context2vec'...\n",
      "remote: Enumerating objects: 30, done.\u001b[K\n",
      "remote: Total 30 (delta 0), reused 0 (delta 0), pack-reused 30\u001b[K\n",
      "Receiving objects: 100% (30/30), 440.45 KiB | 2.31 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/SenticNet/context2vec.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16d72c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3668c058e88a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_helper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_vocab_to_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_glove\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_verb_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProgbar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_helper'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from data_helper import read_vocab_to_dict, build_glove, dataset_iterator, load_verb_count\n",
    "from model import Model\n",
    "from logger import Progbar\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "glove_path = os.path.join(os.path.expanduser(\"~\"), \"data\", \"embeddings\", \"glove\", \"glove.840B.300d.txt\")\n",
    "\n",
    "# read word and verb dict\n",
    "print(\"load dictionary...\")\n",
    "word_dict = read_vocab_to_dict(\"data/word_vocab.txt\")\n",
    "verb_dict = read_vocab_to_dict(\"data/verb_vocab.txt\")\n",
    "verb_vocab_count = load_verb_count(\"data/verb_count.txt\")\n",
    "\n",
    "flags = tf.flags\n",
    "flags.DEFINE_integer(\"neg_sample\", 10, \"number of negative samples\")\n",
    "flags.DEFINE_integer(\"word_dim\", 300, \"word embedding dimension\")\n",
    "flags.DEFINE_integer(\"num_units\", 100, \"number of units for rnn cell and hidden layer of ffn\")\n",
    "flags.DEFINE_integer(\"output_units\", 200, \"number of units for output part\")\n",
    "flags.DEFINE_bool(\"use_pretrained\", True, \"use pretrained word2vec\")\n",
    "flags.DEFINE_bool(\"tune_emb\", True, \"tune pretrained embeddings while training\")\n",
    "flags.DEFINE_string(\"glove_path\", glove_path, \"pretrained glove embeddings path\")\n",
    "flags.DEFINE_string(\"pretrained_context\", \"data/glove_context.npz\", \"pretrained context embedding path\")\n",
    "flags.DEFINE_string(\"pretrained_target\", \"data/glove_target.npz\", \"pretrained target embedding path\")\n",
    "flags.DEFINE_integer(\"vocab_size\", len(word_dict), \"word vocab size\")\n",
    "flags.DEFINE_integer(\"verb_size\", len(verb_dict), \"verb vocab size\")\n",
    "flags.DEFINE_float(\"lr\", 0.001, \"learning_rate\")\n",
    "flags.DEFINE_integer(\"batch_size\", 300, \"batch size\")\n",
    "flags.DEFINE_string(\"dataset\", \"data/dataset.txt\", \"dataset\")\n",
    "flags.DEFINE_integer(\"epochs\", 3, \"epochs\")\n",
    "flags.DEFINE_string(\"ckpt\", \"ckpt/\", \"checkpoint path\")\n",
    "flags.DEFINE_string(\"model_name\", \"concept_prime\", \"model name\")\n",
    "config = flags.FLAGS\n",
    "\n",
    "# initialize with pretrained glove embeddings\n",
    "if not os.path.exists(config.pretrained_context) or not os.path.exists(config.pretrained_target):\n",
    "    build_glove(config.glove_path, config.pretrained_context, config.pretrained_target, word_dict, verb_dict,\n",
    "                config.word_dim)\n",
    "\n",
    "if not os.path.exists(config.ckpt):\n",
    "    os.makedirs(config.ckpt)\n",
    "\n",
    "# training the model\n",
    "print(\"start training...\")\n",
    "sess_config = tf.ConfigProto(allow_soft_placement=True)\n",
    "sess_config.gpu_options.allow_growth = True\n",
    "with tf.Session(config=sess_config) as sess:\n",
    "    # build model\n",
    "    print(\"build model...\")\n",
    "    model = Model(config, verb_vocab_count)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(max_to_keep=1)\n",
    "    for epoch in range(config.epochs):\n",
    "        prog = Progbar(target=int(2.663e8) / config.batch_size)\n",
    "        for i, data in enumerate(dataset_iterator(config.dataset, word_dict, verb_dict, config.batch_size)):\n",
    "            feed_dict = model.get_feed_dict(data, is_train=True, lr=config.lr)\n",
    "            _, losses = sess.run([model.train_op, model.loss], feed_dict=feed_dict)\n",
    "            prog.update(i + 1, [(\"train loss\", losses)])\n",
    "    # save the model\n",
    "    saver.save(sess, config.ckpt + config.model_name, global_step=config.epochs)\n",
    "    # save the trained target embedding\n",
    "    target_emb = sess.run(model.verb_embeddings)\n",
    "    np.savez_compressed(\"data/trained_target_emb.npz\", embeddings=target_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edeae220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
