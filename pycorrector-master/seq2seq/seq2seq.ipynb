{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bbf3c2f",
   "metadata": {},
   "source": [
    "# install all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58213b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "     -------------------------------------- 125.4/125.4 kB 3.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\15197\\anaconda3\\lib\\site-packages (from tensorboardX) (1.20.3)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from tensorboardX) (3.19.1)\n",
      "Installing collected packages: tensorboardX\n",
      "Successfully installed tensorboardX-2.5.1\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20cb4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "     ---------------------------------------- 4.7/4.7 MB 9.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 7.8 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
      "     -------------------------------------- 120.7/120.7 kB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\15197\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\15197\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.0.7)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.3\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4b4a362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "     -------------------------------------- 365.7/365.7 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (2.26.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "     ---------------------------------------- 132.3/132.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\15197\\anaconda3\\lib\\site-packages (from datasets) (1.3.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from datasets) (1.20.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-win_amd64.whl (29 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from datasets) (4.62.3)\n",
      "Collecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2022.8.2-py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.8/140.8 kB 8.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\15197\\anaconda3\\lib\\site-packages (from datasets) (21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from datasets) (0.9.1)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-9.0.0-cp39-cp39-win_amd64.whl (19.6 MB)\n",
      "     ---------------------------------------- 19.6/19.6 MB 5.1 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp39-cp39-win_amd64.whl (554 kB)\n",
      "     -------------------------------------- 554.9/554.9 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.8/95.8 kB 5.3 MB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.1-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.7/56.7 kB 2.9 MB/s eta 0:00:00\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.1-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from aiohttp->datasets) (2.0.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\15197\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->datasets) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\15197\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\15197\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.10.1\n",
      "    Uninstalling fsspec-2021.10.1:\n",
      "      Successfully uninstalled fsspec-2021.10.1\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.4.0 dill-0.3.5.1 frozenlist-1.3.1 fsspec-2022.8.2 multidict-6.0.2 multiprocess-0.70.13 pyarrow-9.0.0 responses-0.18.0 xxhash-3.0.0 yarl-1.8.1\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fbf512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loguru\n",
      "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama>=0.3.4 in c:\\users\\15197\\anaconda3\\lib\\site-packages (from loguru) (0.4.4)\n",
      "Collecting win32-setctime>=1.0.0\n",
      "  Downloading win32_setctime-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Installing collected packages: win32-setctime, loguru\n",
      "Successfully installed loguru-0.6.0 win32-setctime-1.1.0\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install loguru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1c2b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2213d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypinyin\n",
      "  Downloading pypinyin-0.47.1-py2.py3-none-any.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 7.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pypinyin\n",
      "Successfully installed pypinyin-0.47.1\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install pypinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e42bf554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jieba\n",
      "  Downloading jieba-0.42.1.tar.gz (19.2 MB)\n",
      "     --------------------------------------- 19.2/19.2 MB 11.7 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: jieba\n",
      "  Building wheel for jieba (setup.py): started\n",
      "  Building wheel for jieba (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314476 sha256=f6a70343f03619393d65c60f9a5f4e8ae23feb5598d18bf585f7e33797d03e23\n",
      "  Stored in directory: c:\\users\\15197\\appdata\\local\\pip\\cache\\wheels\\7d\\74\\cf\\08c94db4b784e2c1ef675a600b7b5b281fd25240dcb954ee7e\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\15197\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09782ad",
   "metadata": {},
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006b3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../..')\n",
    "from pycorrector.seq2seq.data_reader import *\n",
    "from pycorrector.seq2seq.train import *\n",
    "from pycorrector.seq2seq.infer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3ce4d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--raw_train_path\",\n",
    "                    default=\"../pycorrector/data/cn/sighan_2015/train.tsv\", type=str,\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\",\n",
    "                    )\n",
    "parser.add_argument(\"--dataset\", default=\"sighan\", type=str,\n",
    "                    help=\"Dataset name. selected in the list:\" + \", \".join([\"sighan\", \"cged\"])\n",
    "                    )\n",
    "parser.add_argument(\"--use_segment\", action=\"store_true\", help=\"Whether not to segment train data\")\n",
    "parser.add_argument(\"--do_preprocess\", action=\"store_true\", default=\"True\",help=\"Whether not to preprocess train data\")\n",
    "parser.add_argument(\"--segment_type\", default=\"char\", type=str,\n",
    "                        help=\"Segment data type, selected in list: \" + \", \".join([\"char\", \"word\"]))\n",
    "parser.add_argument(\"--model_name_or_path\",\n",
    "                    default=\"bert-base-chinese\", type=str,\n",
    "                    help=\"Path to pretrained model or model identifier from huggingface.co/models\",\n",
    "                    )\n",
    "parser.add_argument(\"--model_dir\", default=\"output/RNA/\", type=str, help=\"Dir for model save.\")\n",
    "parser.add_argument(\"--arch\", default=\"seq2seq\", type=str,\n",
    "                    help=\"The name of the task to train selected in the list: \" + \", \".join(\n",
    "                        ['seq2seq', 'convseq2seq', 'bertseq2seq']),\n",
    "                    )\n",
    "parser.add_argument(\"--train_path\", default=\"output/train.txt\", type=str, help=\"Train file after preprocess.\")\n",
    "parser.add_argument(\"--test_path\", default=\"output/test.txt\", type=str, help=\"Test file after preprocess.\")\n",
    "parser.add_argument(\"--max_length\", default=128, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. \\n\"\n",
    "                            \"Sequences longer than this will be truncated, sequences shorter padded.\",\n",
    "                    )\n",
    "parser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size.\")\n",
    "parser.add_argument(\"--embed_size\", default=128, type=int, help=\"Embedding size.\")\n",
    "parser.add_argument(\"--hidden_size\", default=128, type=int, help=\"Hidden size.\")\n",
    "parser.add_argument(\"--dropout\", default=0.25, type=float, help=\"Dropout rate.\")\n",
    "parser.add_argument(\"--epochs\", default=1000, type=int, help=\"Epoch num.\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e49c95c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "data = get_data_file(args.raw_train_path, args.use_segment, args.segment_type)[:1000]\n",
    "data1 = get_data_file(\"../pycorrector/data/RNA/train\", args.use_segment, args.segment_type)\n",
    "\n",
    "for i in range(1000):\n",
    "    [a,b] = data1[i]\n",
    "    if a!=b:\n",
    "        print(i)\n",
    "        break\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecdee55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GLY GLU ALA SER PRO VAL ASP PRO LEU ARG PRO VAL VAL ASP ALA SER ILE GLN PRO LEU LEU LYS GLU HIS ARG ILE PRO GLY MET ALA VAL ALA VAL LEU LYS ASP GLY LYS ALA HIS TYR PHE ASN TYR GLY VAL ALA ASN ARG GLU SER GLY ALA GLY VAL SER GLU GLN THR LEU PHE GLU ILE GLY SER VAL SER LYS THR LEU THR ALA THR LEU GLY ALA TYR ALA VAL VAL LYS GLY ALA MET GLN LEU ASP ASP LYS ALA SER ARG HIS ALA PRO TRP LEU LYS GLY SER ALA PHE ASP SER ILE THR MET GLY GLU LEU ALA THR TYR SER ALA GLY GLY LEU PRO LEU GLN PHE PRO GLU GLU VAL ASP SER SER GLU LYS MET ARG ALA TYR TYR ARG GLN TRP ALA PRO VAL TYR SER PRO GLY SER HIS ARG GLN TYR SER ASN PRO SER ILE GLY LEU PHE GLY HIS LEU ALA ALA SER SER LEU LYS GLN PRO PHE ALA PRO LEU MET GLU GLN THR LEU LEU PRO GLY LEU GLY MET HIS HIS THR TYR VAL ASN VAL PRO LYS GLN ALA MET ALA SER TYR ALA TYR GLY TYR SER LYS GLU ASP LYS PRO ILE ARG VAL ASN PRO GLY MET LEU ALA ASP GLU ALA TYR GLY ILE LYS THR SER SER ALA ASP LEU LEU ARG PHE VAL LYS ALA ASN ILE GLY GLY VAL ASP ASP LYS ALA LEU GLN GLN ALA ILE SER LEU THR HIS GLN GLY HIS TYR SER VAL GLY GLY MET THR GLN GLY LEU GLY TRP GLU SER TYR ALA TYR PRO VAL THR GLU GLN THR LEU LEU ALA GLY ASN SER ALA LYS VAL ILE LEU GLU ALA ASN PRO THR ALA ALA PRO ARG GLU SER GLY SER GLN VAL LEU PHE ASN LYS THR GLY SER THR ASN GLY PHE GLY ALA TYR VAL ALA PHE VAL PRO ALA ARG GLY ILE GLY ILE VAL MET LEU ALA ASN ARG ASN TYR PRO ILE GLU ALA ARG ILE LYS ALA ALA HIS ALA ILE LEU ALA GLN LEU ALA GLY',\n",
       " 'GLY LEU ALA SER PRO VAL ASP PRO LEU ARG PRO VAL VAL ASP ALA SER ILE GLN PRO LEU LEU LYS GLU HIS ARG ILE PRO GLY MET ALA VAL ALA VAL LEU LYS ASP GLY LYS ALA HIS TYR PHE ASN TYR GLY VAL ALA ASN ARG GLU SER GLY ALA GLY VAL SER GLU GLN THR LEU PHE GLU ILE GLY SER VAL SER LYS THR LEU THR ALA THR LEU GLY ALA TYR ALA VAL VAL LYS GLY ALA MET GLN LEU ASP ASP LYS ALA SER ARG HIS ALA PRO TRP LEU LYS GLY SER ALA PHE ASP SER ILE THR MET GLY GLU LEU ALA THR TYR SER ALA GLY GLY LEU PRO LEU GLN PHE PRO GLU GLU VAL ASP SER SER GLU LYS MET ARG ALA TYR TYR ARG GLN TRP ALA PRO VAL TYR SER PRO GLY SER HIS ARG GLN TYR SER ASN PRO SER ILE GLY LEU PHE GLY HIS LEU ALA ALA SER SER LEU LYS GLN PRO PHE ALA PRO LEU MET GLU GLN THR LEU LEU PRO GLY LEU GLY MET HIS HIS THR TYR VAL ASN VAL PRO LYS GLN ALA MET ALA SER TYR ALA TYR GLY TYR SER LYS GLU ASP LYS PRO ILE ARG VAL ASN PRO GLY MET LEU ALA ASP GLU ALA TYR GLY ILE LYS THR SER SER ALA ASP LEU LEU ARG PHE VAL LYS ALA ASN ILE GLY GLY VAL ASP ASP LYS ALA LEU GLN GLN ALA ILE SER LEU THR HIS GLN GLY HIS TYR SER VAL GLY GLY MET THR GLN GLY LEU GLY TRP GLU SER TYR ALA TYR PRO VAL THR GLU GLN THR LEU LEU ALA GLY ASN SER ALA LYS VAL ILE LEU GLU ALA ASN PRO THR ALA ALA PRO ARG GLU SER GLY SER GLN VAL LEU PHE ASN LYS THR GLY SER THR ASN GLY PHE GLY ALA TYR VAL ALA PHE VAL PRO ALA ARG GLY ILE GLY ILE VAL MET LEU ALA ASN ARG ASN TYR PRO ILE GLU ALA ARG ILE LYS ALA ALA HIS ALA ILE LEU ALA GLN LEU ALA GLY']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e086b810",
   "metadata": {},
   "source": [
    "# data_reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a9703a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7611ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants associated with the usual special tokens.\n",
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "PAD_TOKEN = '<pad>'\n",
    "class CscDataset(object):\n",
    "    def __init__(self, file_path):\n",
    "        self.data = json.load(open(file_path, 'r', encoding='utf-8'))\n",
    "\n",
    "    def load(self):\n",
    "        data_list = []\n",
    "        for item in self.data:\n",
    "            data_list.append(item['original_text'] + '\\t' + item['correct_text'])\n",
    "        return data_list\n",
    "\n",
    "def create_dataset(path, num_examples=None, split_on_space=False):\n",
    "    if path.endswith('.json'):\n",
    "        d = CscDataset(path)\n",
    "        lines = d.load()\n",
    "    else:\n",
    "        lines = open(path, 'r', encoding='utf-8').read().strip().split('\\n')\n",
    "    word_pairs = [[preprocess_sentence(s, split_on_space) for s in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "\n",
    "def preprocess_sentence(sentence, split_on_space=False):\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    return [SOS_TOKEN] + sentence.split() if split_on_space else list(sentence) + [EOS_TOKEN]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aab244fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "'''\n",
    "class Counter(object):\n",
    "    def __init__(self):\n",
    "        self.counter = {}\n",
    "    def update(self, token):\n",
    "'''    \n",
    "\n",
    "def read_vocab(input_texts, max_size=None, min_count=0):\n",
    "    token_counts = Counter()\n",
    "    special_tokens = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "    '''\n",
    "    for texts in input_texts:\n",
    "        for token in texts:\n",
    "            token_counts.update(token)\n",
    "    '''\n",
    "    for texts in input_texts:\n",
    "        for token in texts:\n",
    "            token_counts.update(token)\n",
    "    # Sort word count by value\n",
    "    count_pairs = token_counts.most_common()\n",
    "    vocab = [k for k, v in count_pairs if v >= min_count]\n",
    "    print(vocab)\n",
    "    # Insert the special tokens to the beginning\n",
    "    vocab[0:0] = special_tokens\n",
    "    full_token_id = list(zip(vocab, range(len(vocab))))[:max_size]\n",
    "    vocab2id = dict(full_token_id)\n",
    "    return vocab2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04dc4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(src_sentences, trg_sentences, src_dict, trg_dict, sort_by_len=True):\n",
    "    \"\"\"vector the sequences.\n",
    "    \"\"\"\n",
    "\n",
    "    out_src_sentences = [[src_dict.get(w, 0) for w in sent] for sent in src_sentences]\n",
    "    out_trg_sentences = [[trg_dict.get(w, 0) for w in sent] for sent in trg_sentences]\n",
    "\n",
    "    # sort sentences by english lengths\n",
    "    def len_argsort(seq):\n",
    "        return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "    # sort length\n",
    "    if sort_by_len:\n",
    "        sorted_index = len_argsort(out_src_sentences)\n",
    "        out_src_sentences = [out_src_sentences[i] for i in sorted_index]\n",
    "        out_trg_sentences = [out_trg_sentences[i] for i in sorted_index]\n",
    "\n",
    "    return out_src_sentences, out_trg_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d076ddef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author:XuMing(xuming624@qq.com)\n",
    "@description: \n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        '''\n",
    "            torch.nn.Embedding(numembeddings, embeddingdim)\n",
    "                * numembeddings代表一共有多少个词\n",
    "                * embedding_dim代表每个词创建一个多少维的向量来表示他\n",
    "        '''\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True, bidirectional=True)\n",
    "        '''\n",
    "            torch.nn.GRU(input_size, hidden_size, num_layers, bias,batch_first,dropout,bidirectional)\n",
    "                * input_size: the number of expected features in the input x\n",
    "                * hidden_size: the number of features in the hidden state h\n",
    "                * batch_first: if True, then (batch, seq, feature), else (seq, batch, feature)\n",
    "                * bidirectional: if True, becomes a bidirectional GRU. Default: False\n",
    "            \n",
    "        '''\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(enc_hidden_size * 2, dec_hidden_size)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        # 将x根据长度来排序\n",
    "        sorted_len, sorted_idx = lengths.sort(0, descending=True)\n",
    "        x_sorted = x[sorted_idx.long()]\n",
    "        embedded = self.dropout(self.embed(x_sorted))\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(),\n",
    "                                                            batch_first=True)\n",
    "        '''\n",
    "            https://zhuanlan.zhihu.com/p/34418001\n",
    "            torch.nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False,enforce_sorted=True)\n",
    "            当我们进行batch个训练数据一起计算的时候，我们会遇到多个训练样例长度不同时的情况，这样我们就会很自然的进行padding，\n",
    "            将短句子padding为跟最长的句子一样\n",
    "            \n",
    "            pytorch中RNN处理变长padding主要用torch.nn.utils.rnn.pack_padded_sequence()以及torch.nn.utils.rnn.pad_packed_sequence()来进行。\n",
    "            \n",
    "            \n",
    "            \n",
    "        '''\n",
    "        packed_out, hid = self.rnn(packed_embedded)\n",
    "        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        out = out[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        hid = torch.cat([hid[-2], hid[-1]], dim=1)\n",
    "        hid = torch.tanh(self.fc(hid)).unsqueeze(0)\n",
    "\n",
    "        return out, hid\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Luong Attention,根据context vectors和当前的输出hidden states，计算输出\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, enc_hidden_size, dec_hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.enc_hidden_size = enc_hidden_size\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "        self.linear_in = nn.Linear(enc_hidden_size * 2, dec_hidden_size, bias=False)\n",
    "        self.linear_out = nn.Linear(enc_hidden_size * 2 + dec_hidden_size, dec_hidden_size)\n",
    "\n",
    "    def forward(self, output, context, mask):\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        # context: batch_size, context_len, 2*enc_hidden_size\n",
    "\n",
    "        batch_size = output.size(0)\n",
    "        output_len = output.size(1)\n",
    "        input_len = context.size(1)\n",
    "\n",
    "        context_in = self.linear_in(context.view(batch_size * input_len, -1)).view(\n",
    "            batch_size, input_len, -1)  # batch_size, context_len, dec_hidden_size\n",
    "\n",
    "        # context_in.transpose(1,2): batch_size, dec_hidden_size, context_len\n",
    "        # output: batch_size, output_len, dec_hidden_size\n",
    "        attn = torch.bmm(output, context_in.transpose(1, 2))\n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        attn.data.masked_fill(mask, -1e6)\n",
    "\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        # batch_size, output_len, context_len\n",
    "\n",
    "        context = torch.bmm(attn, context)\n",
    "        # batch_size, output_len, enc_hidden_size\n",
    "\n",
    "        output = torch.cat((context, output), dim=2)  # batch_size, output_len, hidden_size*2\n",
    "\n",
    "        output = output.view(batch_size * output_len, -1)\n",
    "        output = torch.tanh(self.linear_out(output))\n",
    "        output = output.view(batch_size, output_len, -1)\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.attention = Attention(enc_hidden_size, dec_hidden_size)\n",
    "        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(dec_hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def create_mask(self, x_len, y_len):\n",
    "        # a mask of shape x_len * y_len\n",
    "        max_x_len = x_len.max()\n",
    "        max_y_len = y_len.max()\n",
    "        x_mask = torch.arange(max_x_len, device=x_len.device)[None, :] < x_len[:, None]\n",
    "        y_mask = torch.arange(max_y_len, device=x_len.device)[None, :] < y_len[:, None]\n",
    "        mask = ~ x_mask[:, :, None] * y_mask[:, None, :]\n",
    "        return mask\n",
    "\n",
    "    def forward(self, ctx, ctx_lengths, y, y_lengths, hid):\n",
    "        sorted_len, sorted_idx = y_lengths.sort(0, descending=True)\n",
    "        y_sorted = y[sorted_idx.long()]\n",
    "        hid = hid[:, sorted_idx.long()]\n",
    "\n",
    "        y_sorted = self.dropout(self.embed(y_sorted))  # batch_size, output_length, embed_size\n",
    "\n",
    "        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=True)\n",
    "        out, hid = self.rnn(packed_seq, hid)\n",
    "        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "        output_seq = unpacked[original_idx.long()].contiguous()\n",
    "        hid = hid[:, original_idx.long()].contiguous()\n",
    "\n",
    "        mask = self.create_mask(y_lengths, ctx_lengths)\n",
    "\n",
    "        output, attn = self.attention(output_seq, ctx, mask)\n",
    "        output = F.log_softmax(self.out(output), -1)\n",
    "\n",
    "        return output, hid, attn\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    Seq2Seq, 最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 encoder_vocab_size,\n",
    "                 decoder_vocab_size,\n",
    "                 embed_size,\n",
    "                 enc_hidden_size,\n",
    "                 dec_hidden_size,\n",
    "                 dropout,\n",
    "                 ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = Encoder(vocab_size=encoder_vocab_size,\n",
    "                               embed_size=embed_size,\n",
    "                               enc_hidden_size=enc_hidden_size,\n",
    "                               dec_hidden_size=dec_hidden_size,\n",
    "                               dropout=dropout)\n",
    "        self.decoder = Decoder(vocab_size=decoder_vocab_size,  # len(trg_2_ids),\n",
    "                               embed_size=embed_size,\n",
    "                               enc_hidden_size=enc_hidden_size,\n",
    "                               dec_hidden_size=dec_hidden_size,\n",
    "                               dropout=dropout)\n",
    "\n",
    "    def forward(self, x, x_lengths, y, y_lengths):\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        output, hid, attn = self.decoder(ctx=encoder_out,\n",
    "                                         ctx_lengths=x_lengths,\n",
    "                                         y=y,\n",
    "                                         y_lengths=y_lengths,\n",
    "                                         hid=hid)\n",
    "        return output, attn\n",
    "\n",
    "    def translate(self, x, x_lengths, y, max_length=128):\n",
    "        print(len(x))\n",
    "        encoder_out, hid = self.encoder(x, x_lengths)\n",
    "        preds = []\n",
    "        batch_size = x.shape[0]\n",
    "        attns = []\n",
    "        for i in range(max_length):\n",
    "            output, hid, attn = self.decoder(ctx=encoder_out,\n",
    "                                             ctx_lengths=x_lengths,\n",
    "                                             y=y,\n",
    "                                             y_lengths=torch.ones(batch_size).long().to(y.device),\n",
    "                                             hid=hid)\n",
    "            \n",
    "            y = output.max(2)[1].view(batch_size, 1)\n",
    "            preds.append(y)\n",
    "            \n",
    "            attns.append(attn)\n",
    "        return torch.cat(preds, 1), torch.cat(attns, 1)\n",
    "\n",
    "\n",
    "class LanguageModelCriterion(nn.Module):\n",
    "    \"\"\"\n",
    "    masked cross entropy loss\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LanguageModelCriterion, self).__init__()\n",
    "\n",
    "    def forward(self, input, target, mask):\n",
    "        # input: (batch_size * seq_len) * vocab_size\n",
    "        input = input.contiguous().view(-1, input.size(2))\n",
    "        # target: batch_size * 1\n",
    "        target = target.contiguous().view(-1, 1)\n",
    "        mask = mask.contiguous().view(-1, 1)\n",
    "        output = -input.gather(1, target) * mask\n",
    "        output = torch.sum(output) / torch.sum(mask)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249d787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unk_tokens = [' ', '“', '”', '‘', '’', '琊', '\\n', '…', '—', '擤', '\\t', '֍', '玕', '', '《', '》']\n",
    "\n",
    "\n",
    "def get_errors(corrected_text, origin_text):\n",
    "    sub_details = []\n",
    "    for i, ori_char in enumerate(origin_text):\n",
    "        if i >= len(corrected_text):\n",
    "            continue\n",
    "        if ori_char in unk_tokens:\n",
    "            # deal with unk word\n",
    "            corrected_text = corrected_text[:i] + ori_char + corrected_text[i:]\n",
    "            continue\n",
    "        if ori_char != corrected_text[i]:\n",
    "            sub_details.append((ori_char, corrected_text[i], i, i + 1))\n",
    "    sub_details = sorted(sub_details, key=operator.itemgetter(2))\n",
    "    return corrected_text, sub_details\n",
    "\n",
    "\n",
    "class Inference(object):\n",
    "    def __init__(self, model_dir, arch='convseq2seq',\n",
    "                 embed_size=128, hidden_size=128, dropout=0.25, max_length=128):\n",
    "        logger.debug(\"Device: {}\".format(device))\n",
    "        logger.debug(f'Use {arch} model.')\n",
    "        if arch in ['seq2seq', 'convseq2seq']:\n",
    "            src_vocab_path = os.path.join(model_dir, 'vocab_source.txt')\n",
    "            trg_vocab_path = os.path.join(model_dir, 'vocab_target.txt')\n",
    "            self.src_2_ids = load_word_dict(src_vocab_path)\n",
    "            self.trg_2_ids = load_word_dict(trg_vocab_path)\n",
    "            self.id_2_trgs = {v: k for k, v in self.trg_2_ids.items()}\n",
    "            if arch == 'seq2seq':\n",
    "                self.model = Seq2Seq(encoder_vocab_size=len(self.src_2_ids),\n",
    "                                     decoder_vocab_size=len(self.trg_2_ids),\n",
    "                                     embed_size=embed_size,\n",
    "                                     enc_hidden_size=hidden_size,\n",
    "                                     dec_hidden_size=hidden_size,\n",
    "                                     dropout=dropout).to(device)\n",
    "                model_path = os.path.join(model_dir, 'seq2seq.pth')\n",
    "                logger.debug('Load model from {}'.format(model_path))\n",
    "                self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                trg_pad_idx = self.trg_2_ids[PAD_TOKEN]\n",
    "                self.model = ConvSeq2Seq(encoder_vocab_size=len(self.src_2_ids),\n",
    "                                         decoder_vocab_size=len(self.trg_2_ids),\n",
    "                                         embed_size=embed_size,\n",
    "                                         enc_hidden_size=hidden_size,\n",
    "                                         dec_hidden_size=hidden_size,\n",
    "                                         dropout=dropout,\n",
    "                                         trg_pad_idx=trg_pad_idx,\n",
    "                                         device=device,\n",
    "                                         max_length=max_length).to(device)\n",
    "                model_path = os.path.join(model_dir, 'convseq2seq.pth')\n",
    "                self.model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "                logger.debug('Load model from {}'.format(model_path))\n",
    "                self.model.eval()\n",
    "        elif arch == 'bertseq2seq':\n",
    "            # Bert Seq2seq model\n",
    "            use_cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "            # encoder_type=None, encoder_name=None, decoder_name=None\n",
    "            self.model = Seq2SeqModel(\"bert\", \"{}/encoder\".format(model_dir),\n",
    "                                      \"{}/decoder\".format(model_dir), use_cuda=use_cuda)\n",
    "        else:\n",
    "            logger.error('error arch: {}'.format(arch))\n",
    "            raise ValueError(\"Model arch choose error. Must use one of seq2seq model.\")\n",
    "        self.arch = arch\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def predict(self, sentence_list):\n",
    "        result = []\n",
    "        if self.arch in ['seq2seq', 'convseq2seq']:\n",
    "            for query in sentence_list:\n",
    "                out = []\n",
    "                tokens = query.split()\n",
    "                tokens = [SOS_TOKEN] + tokens + [EOS_TOKEN]\n",
    "                src_ids = [self.src_2_ids[i] for i in tokens if i in self.src_2_ids]\n",
    "\n",
    "                sos_idx = self.trg_2_ids[SOS_TOKEN]\n",
    "                if self.arch == 'seq2seq':\n",
    "                    src_tensor = torch.from_numpy(np.array(src_ids).reshape(1, -1)).long().to(device)\n",
    "                    src_tensor_len = torch.from_numpy(np.array([len(src_ids)])).long().to(device)\n",
    "                    sos_tensor = torch.Tensor([[self.trg_2_ids[SOS_TOKEN]]]).long().to(device)\n",
    "                    translation, attn = self.model.translate(src_tensor, src_tensor_len, sos_tensor, self.max_length)\n",
    "                    translation = [self.id_2_trgs[i] for i in translation.data.cpu().numpy().reshape(-1) if\n",
    "                                   i in self.id_2_trgs]\n",
    "                else:\n",
    "                    src_tensor = torch.from_numpy(np.array(src_ids).reshape(1, -1)).long().to(device)\n",
    "                    translation, attn = self.model.translate(src_tensor, sos_idx)\n",
    "                    translation = [self.id_2_trgs[i] for i in translation if i in self.id_2_trgs]\n",
    "                for word in translation:\n",
    "                    if word != EOS_TOKEN:\n",
    "                        out.append(word)\n",
    "                    else:\n",
    "                        break\n",
    "                corrected_text = ''.join(out)\n",
    "                corrected_text, sub_details = get_errors(corrected_text, query)\n",
    "                result.append([corrected_text, sub_details])\n",
    "        else:\n",
    "            corrected_sents = self.model.predict(sentence_list)\n",
    "            corrected_sents = [i.replace(' ', '') for i in corrected_sents]\n",
    "            for c, s in zip(corrected_sents, sentence_list):\n",
    "                c = c.replace(' ', '')\n",
    "                c, sub_details = get_errors(c, s)\n",
    "                result.append([c, sub_details])\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2dd74a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(arch, train_path, batch_size, embed_size, hidden_size, dropout, epochs,\n",
    "          model_dir, max_length, use_segment, model_name_or_path):\n",
    "    logger.info(\"device: {}\".format(device))\n",
    "    arch = arch.lower()\n",
    "    logger.debug(f'use {arch} model.')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    if arch in ['seq2seq', 'convseq2seq']:\n",
    "        src_vocab_path = os.path.join(model_dir, 'vocab_source.txt')\n",
    "        trg_vocab_path = os.path.join(model_dir, 'vocab_target.txt')\n",
    "\n",
    "        source_texts, target_texts = create_dataset(train_path, split_on_space=True)\n",
    "        logger.debug(\"source_texts:\",source_texts)\n",
    "        #src_2_ids = read_vocab(source_texts)\n",
    "        #trg_2_ids = read_vocab(target_texts)\n",
    "        #print(src_2_ids)\n",
    "        #save_word_dict(src_2_ids, src_vocab_path)\n",
    "        #save_word_dict(trg_2_ids, trg_vocab_path)\n",
    "\n",
    "        src_2_ids = load_word_dict(src_vocab_path)\n",
    "        trg_2_ids = load_word_dict(trg_vocab_path)\n",
    "        \n",
    "        id_2_srcs = {v: k for k, v in src_2_ids.items()}\n",
    "        id_2_trgs = {v: k for k, v in trg_2_ids.items()}\n",
    "        train_src, train_trg = one_hot(source_texts, target_texts, src_2_ids, trg_2_ids, sort_by_len=True)\n",
    "\n",
    "        logger.debug(f'src: {[id_2_srcs[i] for i in train_src[0]]}')\n",
    "        logger.debug(f'trg: {[id_2_trgs[i] for i in train_trg[0]]}')\n",
    "\n",
    "        train_data = gen_examples(train_src, train_trg, batch_size, max_length)\n",
    "\n",
    "        if arch == 'seq2seq':\n",
    "            # Normal seq2seq\n",
    "            model = Seq2Seq(encoder_vocab_size=len(src_2_ids),\n",
    "                            decoder_vocab_size=len(trg_2_ids),\n",
    "                            embed_size=embed_size,\n",
    "                            enc_hidden_size=hidden_size,\n",
    "                            dec_hidden_size=hidden_size,\n",
    "                            dropout=dropout).to(device)\n",
    "            logger.info(model)\n",
    "            loss_fn = LanguageModelCriterion().to(device)\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            train_seq2seq_model(model, train_data, device, loss_fn, optimizer, model_dir, epochs=epochs)\n",
    "        else:\n",
    "            # Conv seq2seq model\n",
    "            trg_pad_idx = trg_2_ids[PAD_TOKEN]\n",
    "            model = ConvSeq2Seq(encoder_vocab_size=len(src_2_ids),\n",
    "                                decoder_vocab_size=len(trg_2_ids),\n",
    "                                embed_size=embed_size,\n",
    "                                enc_hidden_size=hidden_size,\n",
    "                                dec_hidden_size=hidden_size,\n",
    "                                dropout=dropout,\n",
    "                                trg_pad_idx=trg_pad_idx,\n",
    "                                device=device,\n",
    "                                max_length=max_length).to(device)\n",
    "            logger.info(model)\n",
    "            loss_fn = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
    "            optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "            train_convseq2seq_model(model, train_data, device, loss_fn, optimizer, model_dir, epochs=epochs)\n",
    "    elif arch == 'bertseq2seq':\n",
    "        # Bert Seq2seq model\n",
    "        model_args = {\n",
    "            \"reprocess_input_data\": True,\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"max_seq_length\": max_length if max_length else 128,\n",
    "            \"train_batch_size\": batch_size if batch_size else 8,\n",
    "            \"num_train_epochs\": epochs if epochs else 10,\n",
    "            \"save_eval_checkpoints\": False,\n",
    "            \"save_model_every_epoch\": False,\n",
    "            \"silent\": False,\n",
    "            \"evaluate_generated_text\": True,\n",
    "            \"evaluate_during_training\": True,\n",
    "            \"evaluate_during_training_verbose\": True,\n",
    "            \"best_model_dir\": os.path.join(model_dir, 'best_model'),\n",
    "            \"use_multiprocessing\": False,\n",
    "            \"save_best_model\": True,\n",
    "            \"max_length\": max_length if max_length else 128,  # The maximum length of the sequence to be generated.\n",
    "            \"output_dir\": model_dir if model_dir else \"./output/bertseq2seq/\",\n",
    "        }\n",
    "\n",
    "        use_cuda = True if torch.cuda.is_available() else False\n",
    "        # encoder_type=None, encoder_name=None, decoder_name=None\n",
    "        # encoder_name=\"bert-base-chinese\"\n",
    "        model = Seq2SeqModel(\"bert\", model_name_or_path, model_name_or_path, args=model_args, use_cuda=use_cuda)\n",
    "\n",
    "        logger.info('start train bertseq2seq ...')\n",
    "        data = load_bert_data(train_path, use_segment)\n",
    "        logger.info(f'load data done, data size: {len(data)}')\n",
    "        logger.debug(f'data samples: {data[:10]}')\n",
    "        train_data, dev_data = train_test_split(data, test_size=0.1, shuffle=False)\n",
    "\n",
    "        train_df = pd.DataFrame(train_data, columns=['input_text', 'target_text'])\n",
    "        dev_df = pd.DataFrame(dev_data, columns=['input_text', 'target_text'])\n",
    "\n",
    "        def count_matches(labels, preds):\n",
    "            logger.debug(f\"labels: {labels[:10]}\")\n",
    "            logger.debug(f\"preds: {preds[:10]}\")\n",
    "            match = sum([1 if label == pred else 0 for label, pred in zip(labels, preds)])\n",
    "            logger.debug(f\"match: {match}\")\n",
    "            return match\n",
    "\n",
    "        model.train_model(train_df, eval_data=dev_df, matches=count_matches)\n",
    "    else:\n",
    "        logger.error('error arch: {}'.format(arch))\n",
    "        raise ValueError(\"Model arch choose error. Must use one of seq2seq model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6793d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 01:15:08.373 | INFO     | __main__:train:3 - device: cuda\n",
      "2022-09-13 01:15:08.373 | DEBUG    | __main__:train:5 - use seq2seq model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save line size:41487 to output/train.txt\n",
      "save line size:4610 to output/test.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-13 01:15:10.321 | DEBUG    | __main__:train:12 - source_texts:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output/RNA/vocab_source.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15340/1317396084.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0msave_corpus_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Train model with train data file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m train(args.arch,\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15340/3111753000.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(arch, train_path, batch_size, embed_size, hidden_size, dropout, epochs, model_dir, max_length, use_segment, model_name_or_path)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m#save_word_dict(trg_2_ids, trg_vocab_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0msrc_2_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_word_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_vocab_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtrg_2_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_word_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrg_vocab_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\bio\\Bioinformatic\\pycorrector-master\\seq2seq\\..\\pycorrector\\seq2seq\\data_reader.py\u001b[0m in \u001b[0;36mload_word_dict\u001b[1;34m(save_path)\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mdict_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output/RNA/vocab_source.txt'"
     ]
    }
   ],
   "source": [
    "if args.do_preprocess:\n",
    "    # Preprocess\n",
    "    data_list = []\n",
    "    os.makedirs(args.model_dir, exist_ok=True)\n",
    "    '''\n",
    "    if args.dataset == 'sighan':\n",
    "        data_list.extend(get_data_file(args.raw_train_path, args.use_segment, args.segment_type))\n",
    "    else:\n",
    "        data_list.extend(parse_xml_file(args.raw_train_path, args.use_segment, args.segment_type))\n",
    "    '''\n",
    "    data_list.extend(get_data_file(\"../pycorrector/data/RNA/train\", args.use_segment, args.segment_type))\n",
    "    if data_list:\n",
    "        save_corpus_data(data_list, args.train_path, args.test_path)\n",
    "# Train model with train data file\n",
    "train(args.arch,\n",
    "        args.train_path,\n",
    "        args.batch_size,\n",
    "        args.embed_size,\n",
    "        args.hidden_size,\n",
    "        args.dropout,\n",
    "        args.epochs,\n",
    "        args.model_dir,\n",
    "        args.max_length,\n",
    "        args.use_segment,\n",
    "        args.model_name_or_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28819632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-12 22:42:00.156 | DEBUG    | __main__:__init__:22 - Device: cpu\n",
      "2022-09-12 22:42:00.157 | DEBUG    | __main__:__init__:23 - Use seq2seq model.\n",
      "2022-09-12 22:42:00.163 | DEBUG    | __main__:__init__:38 - Load model from output/RNA/seq2seq.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "input  : MET LYS LYS LEU GLN ILE ALA VAL GLY ILE ILE ARG ASN GLU ASN ASN GLU ILE PHE ILE THR ARG ARG ALA ALA ASP ALA HIS MET ALA ASN LYS LEU GLU PHE PRO GLY GLY LYS ILE GLU MET GLY GLU THR PRO GLU GLN ALA VAL VAL ARG GLU LEU GLN GLU GLU VAL GLY ILE THR PRO GLN HIS PHE SER LEU PHE GLU LYS LEU GLU TYR GLU PHE PRO ASP ARG HIS ILE THR LEU TRP PHE TRP LEU VAL GLU ARG TRP GLU GLY GLU PRO TRP GLY LYS GLU GLY GLN PRO GLY GLU TRP MET SER LEU VAL GLY LEU ASN ALA ASP ASP PHE PRO PRO ALA ASN GLU PRO VAL ILE ALA LYS LEU LYS ARG LEU\n",
      "predict: MET LYS LYS LEU GLN ILE ALA VAL GLY ILE ILE ARG ASN GLU ASN ASN GLU ILE PHE ILE THR ARG ARG ALA ALA ASP ALA HIS MET ALA ASN LYS LEU GLU PHE PRO GLY GLY LYS ILE GLU MET GLY GLU THR PRO GLU GLN ALA VAL VAL ARG GLU LEU GLN GLU GLU VAL GLY ILE THR PRO GLN HIS PHE SER LEU PHE GLU LYS LEU GLU TYR GLU PHE PRO ASP ARG HIS ILE THR LEU TRP PHE TRP LEU VAL GLU ARG TRP GLU GLY GLU PRO TRP GLY LYS GLU GLY GLN PRO GLY GLU TRP MET SER LEU VAL GLY LEU ASN ALA ASP ASP PHE PRO PRO ALA ASN GLU PRO VAL ILE ALA LYS LEU LYS ARG []\n",
      "\n",
      "input  : ASP ALA ALA ALA ASP ALA SER LYS ARG PHE SER ASP ALA THR TYR PRO ILE ALA GLU LYS PHE ASP TRP GLY GLY SER SER ALA ILE ALA LYS TYR ILE ALA ASP ALA SER ALA GLY ASN PRO ARG GLN ALA ALA LEU ALA VAL GLU LYS LEU LEU GLU VAL GLY LEU THR MET ASP PRO LYS LEU VAL ARG ALA ALA VAL GLU ALA HIS SER LYS ALA LEU ASP SER ALA LYS LYS ASN ALA LYS LEU MET ALA SER LYS GLU ASP PHE ALA ALA VAL ASN GLU ALA LEU ALA ARG MET ILE ALA SER ALA ASP LYS GLN LYS PHE ALA ALA LEU ARG THR ALA PHE PRO GLU SER ARG GLU LEU GLN GLY LYS LEU PHE ALA GLY ASN ASN ALA PHE GLU ALA GLU LYS ALA TYR ASP SER PHE LYS ALA LEU THR SER ALA VAL ARG ASP ALA SER ILE ASN GLY ALA LYS ALA PRO VAL ILE ALA GLU ALA ALA ARG ALA GLU ARG TYR VAL GLY ASP GLY PRO VAL GLY ARG ALA ALA LYS LYS PHE SER GLU ALA THR TYR PRO ILE MET ASP LYS LEU ASP TRP GLY LYS SER PRO GLU ILE SER LYS TYR ILE GLU THR ALA SER ALA LYS ASN PRO LYS MET MET ALA ASP GLY ILE ASP LYS THR LEU GLU VAL ALA LEU THR MET ASN GLN ASN ALA ILE ASN ASP ALA VAL PHE ALA HIS VAL ARG ALA ILE LYS GLY ALA LEU ASN THR PRO GLY LEU VAL ALA GLU ARG ASP ASP PHE ALA ARG VAL ASN LEU ALA LEU ALA LYS MET ILE ALA THR ALA ASP PRO ALA LYS PHE LYS ALA LEU LEU THR ALA PHE PRO GLY ASN ALA ASP LEU GLN MET ALA LEU PHE ALA ALA ASN ASN PRO GLU GLN ALA LYS ALA ALA TYR GLU THR PHE VAL ALA LEU THR SER ALA VAL ALA SER SER THR\n",
      "predict: ASP ALA ALA ALA ASP ALA SER LYS ARG PHE SER ASP ALA THR TYR PRO ILE ALA GLU LYS PHE ASP TRP GLY GLY SER SER ALA ILE ALA LYS TYR ILE ALA ASP ALA SER ALA GLY ASN PRO ARG GLN ALA ALA LEU ALA VAL GLU LYS LEU LEU GLU VAL GLY LEU THR MET ASP PRO LYS LEU VAL ARG ALA ALA VAL GLU ALA HIS SER LYS ALA LEU ASP SER ALA LYS LYS ASN ALA LYS LEU MET ALA SER LYS GLU ASP PHE ALA ALA VAL ASN GLU ALA LEU ALA ARG MET ILE ALA SER ALA ASP LYS GLN LYS PHE ALA ALA LEU ARG THR ALA PHE PRO GLU SER ARG GLU LEU GLN GLY LYS LEU PHE ALA []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "m = Inference(args.model_dir,\n",
    "                  args.arch,\n",
    "                  embed_size=args.embed_size,\n",
    "                  hidden_size=args.hidden_size,\n",
    "                  dropout=args.dropout,\n",
    "                  max_length=args.max_length\n",
    "                  )\n",
    "inputs = [\n",
    "    'MET LYS LYS LEU GLN ILE ALA VAL GLY ILE ILE ARG ASN GLU ASN ASN GLU ILE PHE ILE THR ARG ARG ALA ALA ASP ALA HIS MET ALA ASN LYS LEU GLU PHE PRO GLY GLY LYS ILE GLU MET GLY GLU THR PRO GLU GLN ALA VAL VAL ARG GLU LEU GLN GLU GLU VAL GLY ILE THR PRO GLN HIS PHE SER LEU PHE GLU LYS LEU GLU TYR GLU PHE PRO ASP ARG HIS ILE THR LEU TRP PHE TRP LEU VAL GLU ARG TRP GLU GLY GLU PRO TRP GLY LYS GLU GLY GLN PRO GLY GLU TRP MET SER LEU VAL GLY LEU ASN ALA ASP ASP PHE PRO PRO ALA ASN GLU PRO VAL ILE ALA LYS LEU LYS ARG LEU',\n",
    "    'ASP ALA ILE ALA ASP ALA SER LYS ARG PHE SER ASP ALA THR TYR PRO ILE ALA GLU LYS PHE ASP TRP GLY GLY SER SER ALA ILE ALA LYS TYR ILE ALA ASP ALA SER ALA GLY ASN PRO ARG GLN ALA ALA LEU ALA VAL GLU LYS LEU LEU GLU VAL GLY LEU THR MET ASP PRO LYS LEU VAL ARG ALA ALA VAL GLU ALA HIS SER LYS ALA LEU ASP SER ALA LYS LYS ASN ALA LYS LEU MET ALA SER LYS GLU ASP PHE ALA ALA VAL ASN GLU ALA LEU ALA ARG MET ILE ALA SER ALA ASP LYS GLN LYS PHE ALA ALA LEU ARG THR ALA PHE PRO GLU SER ARG GLU LEU GLN GLY LYS LEU PHE ALA GLY ASN ASN ALA PHE GLU ALA GLU LYS ALA TYR ASP SER PHE LYS ALA LEU THR SER ALA VAL ARG ASP ALA SER ILE ASN GLY ALA LYS ALA PRO VAL ILE ALA GLU ALA ALA ARG ALA GLU ARG TYR VAL GLY ASP GLY PRO VAL GLY ARG ALA ALA LYS LYS PHE SER GLU ALA THR TYR PRO ILE MET ASP LYS LEU ASP TRP GLY LYS SER PRO GLU ILE SER LYS TYR ILE GLU THR ALA SER ALA LYS ASN PRO LYS MET MET ALA ASP GLY ILE ASP LYS THR LEU GLU VAL ALA LEU THR MET ASN GLN ASN ALA ILE ASN ASP ALA VAL PHE ALA HIS VAL ARG ALA ILE LYS GLY ALA LEU ASN THR PRO GLY LEU VAL ALA GLU ARG ASP ASP PHE ALA ARG VAL ASN LEU ALA LEU ALA LYS MET ILE ALA THR ALA ASP PRO ALA LYS PHE LYS ALA LEU LEU THR ALA PHE PRO GLY ASN ALA ASP LEU GLN MET ALA LEU PHE ALA ALA ASN ASN PRO GLU GLN ALA LYS ALA ALA TYR GLU THR PHE VAL ALA LEU THR SER ALA VAL ALA SER SER THR'\n",
    "]\n",
    "outputs = m.predict(inputs)\n",
    "\n",
    "for a, b in zip(inputs, outputs):\n",
    "    print('input  :', a)\n",
    "    print('predict:', b[0], b[1])\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3cd0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf5ef44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
